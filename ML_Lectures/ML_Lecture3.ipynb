{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lecture 3 \n",
    "\n",
    "### **General Framework for network/model training**:\n",
    "* In principle networks can approximate any function however in practice it is easier if pre-processing if applied first.\n",
    "\n",
    "* Pre-processing need not be optimal but applying human intelligence can enhance the performance of the **AI** in network trainig.\n",
    "\n",
    "* Post-processing often only involves passively re-mapping the network output into its raw form.\n",
    "\n",
    "Input data $\\rightarrow$ Pre-processing $\\rightarrow$  Network/ model optimization $\\rightarrow$ Post-processing $\\rightarrow$  Output data \n",
    "\n",
    "### **Pre-processing data**:\n",
    "\n",
    "#### Input normalisation:\n",
    "- Normalse data to be of the same scale and have 0 mean.\n",
    "- Useful for network training, and visual inspection.\n",
    "\n",
    "#### Dimensionality reduction:\n",
    "- Use math to efficiently reduce No. of dimensions.\n",
    "- Define a few variables that are combinations of the raw variables and account for most of the variance. \n",
    "- (Some information will be lost).\n",
    "\n",
    "#### Feature selection: \n",
    "- Include/exclude attributes in the data without changing them.\n",
    "- Automatically or manually.\n",
    "- (Some information will be lost).\n",
    "\n",
    "#### Feature extraction:\n",
    "- Use combination of input variables.\n",
    "- This approach can incorporate 1,2,3.\n",
    "\n",
    "### **Character Recognition Example**:\n",
    "For a 256 x 256 character we have 65536 pixels. One input for each pixel is bad for many reasons:\n",
    "- Poor **generalisation**: Data set would have to be vast to be able to properly constrain all the parameters (**Curse of Dimensionality**).\n",
    "- Computational Cost: Takes a long time to train.\n",
    "\n",
    "### **Input Normalisation**:\n",
    "\n",
    "If variation and scale of some variables is much greater than variation in others, this can bias feature extraction, and hinder model-fiting.\n",
    "\n",
    "Therefore, pre-process data to give 0 mean and unit variance to each variable via simple transformation: \n",
    "- $x \\rightarrow \\frac{x- \\mu}{\\sigma}$\n",
    "- where $\\mu$ is the mean and $\\sigma$ is the standard deviation.\n",
    "- (Individually for each variable).\n",
    "\n",
    "Normalisation, as well as improving the process of machine learning, is useful for interpretation of models.\n",
    "<br> It will maintain correlations.\n",
    "\n",
    "### **Whitening**:\n",
    "- Often optimal to redefine features during pre-processing, no network training/ model fitting is applied to features that are not correlated. This is called **whitening**.\n",
    "- Often achieved during dimensionality reduction - e.g. PCA.\n",
    "\n",
    "Whitening defines a new frame of reference so that in that new frame the features are no longer correlated.\n",
    "\n",
    "#### Formula: \n",
    "- $x' = \\Lambda^{-\\frac{1}{2}}U^T(x - \\mu)$\n",
    "- Where $U$ is a matrix whose columns are the eigenvectors $u_i$ of $\\Sigma$, the covariance matrix of the data, and $\\Lambda$ a matrix with the corresponding eigenvalues $\\lambda_i$ on the diagonal and $\\mu$ is the mean of the data.\n",
    "\n",
    "### **Eigenvectors and eigenvalues**:\n",
    "Given a M x M matrix A if: $A\\underline x = \\lambda\\underline x$ for some scalar $\\lambda \\neq 0, then <u>x</u> is an eigenvector with eigenvalue $\\lambda$.\n",
    "<br>\n",
    "If $A\\underline x = \\lambda\\underline x$ then $A2\\underline x = \\lambda2\\underline x$ it follows <u>x</u> is not unique so it is common to scale <u>x</u> to unit length.\n",
    "* Intuition: The direction of <u>x</u> is unchanged by being transformed by A so it \"reflects\" the principal axis of transformation.\n",
    "\n",
    "### **Eigenvector Facts**:\n",
    "* If A is symmetric (true if A is the covariance matrix), the eigenvectors $u$ will be orthogonal and unit length. And the complete set of eigenvectors can be used as a new frame of reference for the data.\n",
    "* $\\underline x = \\sum\\limits_{i=1}^{d} z_i \\underline u_i$\n",
    "* Expressed in the new coordinate system the matrix becomes diagonal, with the eigenvalues down the diagonal.\n",
    "\n",
    "### **Covariance Matrix**:\n",
    "* Covariance Matrix: \n",
    "    * $\\Sigma_{ij} = \\frac{1}{N}\\sum\\limits_{n=1}^{N}(x_{i,n} - \\overline x_i)(x_{j,n} - \\overline x_j)$\n",
    "* Diagonal terms are the variances of each feature:\n",
    "    - $\\Sigma_{ii} = \\frac{1}{N}\\sum\\limits_{n=1}^{N}(x_{i,n} - \\overline x_i)^2$\n",
    "\n",
    "Off-diagonal terms are the covariances between pairs of features.<br> Covariance is a measure of correlation.\n",
    "\n",
    "Eigenvalues of the covariance matrix are equal to the variances in the frame of reference in which there are no correlations.\n",
    "\n",
    "### **Input Normalizations - some variations**:\n",
    "* Input normalization best for Gaussian (normal) distributed data (This assumption does not always hold).\n",
    "\n",
    "* If a variable is bounded, you might want to transform the range to a uniform scale (0 to 1 e.g.) or binarise (this will lose information however).\n",
    "\n",
    "* Sometimes logarithmic transformation is needed is data is skewed.\n",
    "\n",
    "### **To Summise**:\n",
    "* **Input Normalization** aids visualisation - usually improves performance of model training.\n",
    "\n",
    "* **Dimensionality Reduction/ Feature Selection/ Feature Extraction**:\n",
    "    * **Reduces overfitting** - Less redundant data means less opportunity to make decisions based on noise.\n",
    "\n",
    "    * **Improves Accuracy** - Less misleading data means modeling accuracy improves.\n",
    "\n",
    "    * **Reduces Training Time** - Fewer data points reduce algo. complexity and algos. train faster."
   ]
  }
 ]
}