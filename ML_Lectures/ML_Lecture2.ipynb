{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lecture 2\n",
    "\n",
    "### **Instances and Instance Spaces:**\n",
    "* Data consists of a set of instances. Each instance represents an object of interest.\n",
    "\n",
    "* Set of all possible instances is the **instance space**. \n",
    "    - e.g. set of all email messages written in English\n",
    "    - set of human faces.\n",
    "\n",
    "* Notation:\n",
    "    - Instance Space is denoted by $X$.\n",
    "    - An individual instance will be denoted by $x$.\n",
    "    - $x \\in X$\n",
    "\n",
    "### **Labels and Label Spaces:**\n",
    "* In supervised problems, each instance is associated with a label.\n",
    "\n",
    "* Set of all labels is called **label space**.\n",
    "    - class labels $C$ in <u> classification</u> tasks. e.g. $C$ = {frogs, badgers}.\n",
    "    - real numbers $\\subseteq R$ in <u> regression</u> tasks. e.g. pixel coordinate.\n",
    "\n",
    "* Notation:\n",
    "    - arbitary label space denoted by $Y$.\n",
    "    - Labelling function $f : X \\rightarrow Y$ maps from **instances** to **labels**.\n",
    "    - Label associated with a given instance $x$ denoted by $f(x)$.\n",
    "\n",
    "### **Binary Classification:**\n",
    "* Simplest case deals with just **2** class labels.\n",
    "    - positive ( + or +1 )\n",
    "    - negative ( - or -1 )\n",
    "\n",
    "* Convention - the class of interest is **positive**.\n",
    "\n",
    "* Binary classification task is to label instances with one or other of the class labels.\n",
    "\n",
    "* Can also be understood as **concept identification**.\n",
    "    - e.g. identifying faces in photographs.\n",
    "\n",
    "### **Learning a classifier:**\n",
    "* In practice the true classification of a function $f$ is unknown.\n",
    "\n",
    "* We have a **training set** of **labelled instances**, that is ( $x, f(x)$ ).\n",
    "    - A set of manually annotated examples.\n",
    "\n",
    "* These examples are used to learn a model $\\hat{f}:  X \\rightarrow C$\n",
    "\n",
    "* $\\hat{f}$ approximates the true classification function $f$.\n",
    "    - Should follow the training examples closely.\n",
    "    - Should generalise to whole of the instance space $X$.\n",
    "\n",
    "### **Binary Classification - Assessing Performance:**\n",
    "* For a **learned** binary classifier, $\\hat{f}$ approximates the true classification function $f$.\n",
    "    - $\\hat{f}$ will not exactly be the same function as $f$.\n",
    "    - $\\hat{f}$ will make errors in assigning class labels to instances.\n",
    "\n",
    "### **Training/Validation Split** \n",
    "* Most important is whether the classifer has learned to <u>generalize</u>.\n",
    "\n",
    "* Evalutated by assessing classification accuracy on a **validation set**.\n",
    "    - A set of labelled data that <u> was not</u> used for training.\n",
    "    - Assumed that data was chosen randomly from a set of images sharing common characteristics.\n",
    "    - Often referred to as **independently** and **identically distributed** (or iid).\n",
    "    - If validation set is unusual in some way, it will be a poor measure of classifier quality.\n",
    "\n",
    "* Penalises model <u> overfitting</u> - i.e. just understanding the training set really well.\n",
    "\n",
    "### **K-Fold Cross-Validation**:\n",
    "* Dataset split into K sections.\n",
    "\n",
    "* Each run (K total) one fold is removed from the total sections (training set) and used to validate/test the model. \n",
    "\n",
    "* Typically average is taken of the accuracies for an idea of expected accuracy.\n",
    "\n",
    "* Usually 5/7 folds are used however if model takes a long time to train a smaller K value is used.\n",
    "\n",
    "### **Peaking and maintaining a test set**:\n",
    "* Validation sets are useful however it means we might be making model choices based on the validation set.\n",
    "\n",
    "* For this reason a seperate test set is sometimes used to evaluate final performance (Test set not looked at till the end).\n",
    "\n",
    "* Useful when building real-life systems and need to give a predication on its accuracy. \n",
    "\n",
    "* Mixing the training/validation/testing datasets is called **peeking** - results in an over-inflation of goodness of our model.\n",
    "\n",
    "### **Data Insights**:\n",
    "* Rubbish in $\\rightarrow$ Rubbish out. ML problems require curation of the dataset.\n",
    "\n",
    "* Inspecting data manually - plotting / dimensionality reduction tools - can help to understand.\n",
    "\n",
    "* Data labels <u> always</u> contain mistakes or aimbiguities, so dont blindly trust them.\n",
    "\n",
    "* Always choose your train/test/split randomly otherwise odd differences may be introduced. e.g. first half of dataset may only contain cats.\n",
    "\n",
    "### **Binary Classification**:\n",
    "#### Assessing Performance:\n",
    "* Use **contingency table** / **confusion matrix**:\n",
    "\n",
    "| | **Predicted +** | **Predicted -**| | \n",
    "|---:|:---:|:---:|:---|\n",
    "**Actual +**| 30 | 20| **50**|\n",
    "**Actual -**| 10 | 90| **100**|\n",
    "| | **40**| **110**| **150**|\n",
    "\n",
    "#### Accuracy and Error:\n",
    "* **Accuracy** - Proportion of correctly predicted instances.\n",
    "* **Error** - Proportion of incorrectly predicted instances.\n",
    "\n",
    "From above example:\n",
    "* the accuracy on our validation set of 150 instances is (30 + 90)/150 = 0.8 therefore accuracy of 80%.\n",
    "* Error rate on validation set: (10 + 20)/150 = 0.2 -> (20%).\n",
    "\n",
    "#### More Useful Metrics:\n",
    "Whilst accuracy is important it is not always realistic that both classes are of equal importance. E.g. facial recognition system:\n",
    "* Take the +ve class as one identity and the -ve class the remainder identities. \n",
    "* Trade off between always correctly identifying the +ve class and mis-identifing the -ve class.\n",
    "* On the side of caution i.e. always correctly find the +ve class this creates the chance of incorrectly finding people from the -ve class.\n",
    "    - Current issue with facial recognition and can be met with heavily negative response.\n",
    "\n",
    "E.g. medical diagnosis system:\n",
    "* +ve class as bad cancer (requires surgery), -ve class benign (no surgery).\n",
    "* Trade off between always correctly correctly identifying the +ve class and mis-identifying the -ve class.\n",
    "* etc.\n",
    "\n",
    "#### Per-Class Accuracy:\n",
    "* **True positive rate**: Proportion of correctly predicted +ve instances.\n",
    "* **True negative rate**: Proportion of correctly predicted -ve instances.\n",
    "\n",
    "Using the same matrix:\n",
    "\n",
    "| | **Predicted +** | **Predicted -**| | \n",
    "|---:|:---:|:---:|:---|\n",
    "**Actual +**| 30 | 20| **50**|\n",
    "**Actual -**| 10 | 90| **100**|\n",
    "| | **40**| **110**| **150**|\n",
    "\n",
    "TPR (***Sensitivity***) on validation set is 30/50 = 0.6.<br>TNR (***Specificty***) on validation set is 90/100 = 0.9.\n",
    "\n",
    "#### Per-Class Inaccuracy:\n",
    "* **False positive rate**: Proportion of incorrectly predicted -ive instances.\n",
    "* **False negative rate**: Proportion of incorrectly predicted +ve instances.\n",
    "\n",
    "FPR on validation set is 10/100 = 0.1.\n",
    "<br>FNR on validation set is 20/50 = 0.4.\n",
    "\n",
    "#### Precision and Recall: \n",
    "* **Precision**: Proportion of +ve predicitions that are correct.\n",
    "* **Recall**: Proportion of +ve *instances* that are predicted (**TPR**)\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Multi-Class Classification**: \n",
    "* One-versus-rest (**OVR**): learn $k$ different binary classifiers:\n",
    "    - First separates $C_{1}$ from $C_{2},...,C_{k}$\n",
    "    - Second separates $C_{2}$ from $C_{1},C_{3},...C_{k}$ etc.\n",
    "    - i-th classifier separates $C_{i}$ from all other classes.\n",
    "* One-versus-one (**OVO**): learn $k(k-1)/2$ binary classifiers:\n",
    "    - one classifier for each pair of different classes.\n",
    "\n",
    "#### OVR and OVO:\n",
    "* The OVR and OVO strategies provide a way of combining predictions of binary classifiers.\n",
    "    - OVR: relatively efficient as train just $k$ classifiers.\n",
    "    - OVO: $k(k-1)/2$ classifiers, but less data used in each case.\n",
    "* However a prediction still needs to be made:\n",
    "    - Basic idea is to use the binary predictions as votes for classes and the class with the most \"votes\" wins.\n",
    "\n",
    "#### OVO Example:\n",
    "Suppose 4 classes: $C_1,C_2,C_3,C_4$ and so for OVO there will be $4 \\cdot (4-1)/2 = 6$ classifiers.\n",
    "\n",
    "Using the classifiers to vote:\n",
    "\n",
    "| | $BC_1$| $BC_2$ | $BC_3$ | $BC_4$ | $BC_5$ | $BC_6$ | Votes |\n",
    "|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|$C_1$| + | - | - | 0 | 0 | 0 | 1 |\n",
    "|$C_2$| -|0|0|-|-|0|0|\n",
    "|$C_3$| 0|+|0|+|0|+|3|\n",
    "|$C_4$| 0|0|+|0|+|-|2|\n",
    "\n",
    "- Issue arrises if classes get equal votes.\n",
    "    - If the classifier outputs scores or probabilities:\n",
    "        - loss-based decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
